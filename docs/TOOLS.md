# üõ†Ô∏è RAFT Toolkit - Command Line Tools

The RAFT Toolkit includes several powerful command-line tools for evaluating and analyzing datasets generated by the main toolkit. These tools are automatically installed as console commands when you install the package.

## üì¶ Installation

After installing the RAFT Toolkit, all tools are available as console commands:

```bash
pip install raft-toolkit
```

## üîß Available Tools

### 1. `raft-eval` - Dataset Evaluation Tool

**Purpose**: Evaluate language model performance on RAFT-generated datasets with parallel processing and detailed analytics.

**Usage**:
```bash
raft-eval --question-file questions.jsonl [OPTIONS]
```

**Options**:
- `--question-file` (required): Path to JSONL file containing evaluation questions
- `--answer-file`: Output file for generated answers (default: `answer.jsonl`)
- `--model`: Model for evaluation (default: `gpt-4`)
- `--input-prompt-key`: Input column name (default: `instruction`)
- `--output-answer-key`: Output column name (default: `answer`)
- `--workers`: Number of worker threads (default: `1`)

**Examples**:
```bash
# Basic evaluation
raft-eval --question-file dataset.jsonl

# Multi-threaded evaluation with custom model
raft-eval --question-file dataset.jsonl --model gpt-3.5-turbo --workers 8

# Custom column mapping
raft-eval --question-file dataset.jsonl --input-prompt-key question --output-answer-key response
```

### 2. `raft-answer` - Answer Generation Tool

**Purpose**: Generate answers for evaluation datasets using various language models with batch processing.

**Usage**:
```bash
raft-answer [OPTIONS]
```

**Options**:
- `--input`: Input JSONL file (default: `input.jsonl`)
- `--output`: Output JSONL file (default: `output.jsonl`)
- `--workers`: Number of worker threads (default: `1`)
- `--model`: Model name for answer generation (default: `gpt-4`)
- `--deployment`: Deployment name for Azure OpenAI (default: `gpt-4`)
- `--system-prompt-key`: System prompt type (default: `gpt`)
- `--templates`: Template directory path (default: `./`)
- `--count`: Number of questions to process (default: `-1` for all)

**Examples**:
```bash
# Basic answer generation
raft-answer --input questions.jsonl --output answers.jsonl

# High-throughput processing
raft-answer --input dataset.jsonl --output results.jsonl --workers 10 --model gpt-3.5-turbo

# Process subset of questions
raft-answer --input large_dataset.jsonl --count 100
```

### 3. `raft-pfeval-chat` - PromptFlow Chat Evaluation

**Purpose**: Advanced evaluation using Microsoft PromptFlow framework for chat-format responses.

**Usage**:
```bash
raft-pfeval-chat [OPTIONS]
```

**Options**:
- `--input`: Input JSONL file (default: `input.jsonl`)
- `--output`: Output results file (default: `output.jsonl`)
- `--mode`: Evaluation mode: `local` or `remote` (default: `local`)
- `--model`: Model to evaluate (default: `gpt-4`)
- `--score-model`: Model for scoring (default: `gpt-35-instruct`)
- `--deployment`: Deployment name (default: `gpt-4`)

**Examples**:
```bash
# Local chat evaluation
raft-pfeval-chat --input chat_data.jsonl --output results.json

# Remote evaluation with Azure AI Studio
raft-pfeval-chat --input data.jsonl --mode remote --score-model gpt-4
```

### 4. `raft-pfeval-completion` - PromptFlow Completion Evaluation

**Purpose**: Evaluate completion-format responses using PromptFlow evaluators.

**Usage**:
```bash
raft-pfeval-completion [OPTIONS]
```

**Options**:
- `--input`: Input JSONL file (default: `input.jsonl`)
- `--output`: Output results file (default: `output.jsonl`)
- `--mode`: Evaluation mode: `local` or `remote` (default: `local`)
- `--model`: Model to evaluate (default: `gpt-4`)
- `--deployment`: Deployment name (default: `gpt-4`)

**Examples**:
```bash
# Completion evaluation
raft-pfeval-completion --input completions.jsonl --output scores.json

# Remote evaluation
raft-pfeval-completion --input data.jsonl --mode remote
```

### 5. `raft-pfeval-local` - Local PromptFlow Evaluation

**Purpose**: Offline evaluation using PromptFlow without API calls for basic metrics.

**Usage**:
```bash
raft-pfeval-local [OPTIONS]
```

**Options**:
- `--input`: Input JSONL file (default: `input.jsonl`)
- `--output`: Output results file (default: `output.jsonl`)
- `--mode`: Evaluation mode (default: `local`)
- `--workers`: Number of worker threads (default: `1`)
- `--score-model`: Model for scoring (default: `gpt-35-instruct`)

**Examples**:
```bash
# Local evaluation with multiple workers
raft-pfeval-local --input data.jsonl --workers 4

# Custom scoring model
raft-pfeval-local --input data.jsonl --score-model gpt-4
```

## üîß Environment Configuration

Set up your environment variables for the tools:

```bash
# OpenAI Configuration
export OPENAI_API_KEY="your_openai_api_key"
export OPENAI_API_BASE_URL="https://api.openai.com/v1"

# Azure OpenAI Configuration (optional)
export AZURE_OPENAI_ENABLED="false"
export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com/"
export AZURE_OPENAI_KEY="your_azure_openai_key"
export AZURE_OPENAI_API_VERSION="2024-02-01"

# Evaluation Configuration
export EVAL_MODEL="gpt-4"
export EVAL_WORKERS="4"

# PromptFlow Configuration (for Azure PromptFlow tools)
export SCORE_AZURE_OPENAI_ENDPOINT="https://your-score-endpoint.openai.azure.com/"
export SCORE_AZURE_OPENAI_API_KEY="your_score_key"
export SCORE_OPENAI_API_VERSION="2024-02-01"
export SCORE_AZURE_OPENAI_DEPLOYMENT="gpt-4"

# Azure AI Project Configuration
export GROUNDEDNESS_SUB_ID="your_subscription_id"
export GROUNDEDNESS_GROUP="your_resource_group"
export GROUNDEDNESS_PROJECT_NAME="your_project_name"
```

## üìä Data Formats

### Input Format (Questions)
```json
{"instruction": "What is artificial intelligence?", "context": "AI is a field of computer science..."}
{"instruction": "How do neural networks work?", "context": "Neural networks consist of layers..."}
```

### Output Format (Answers)
```json
{"instruction": "What is artificial intelligence?", "answer": "Artificial intelligence is...", "model": "gpt-4"}
{"instruction": "How do neural networks work?", "answer": "Neural networks work by...", "model": "gpt-4"}
```

## üöÄ Complete Workflow Example

```bash
# 1. Generate dataset with main RAFT toolkit
raft --datapath document.pdf --output evaluation_data

# 2. Generate answers using the answer tool
raft-answer --input evaluation_data/questions.jsonl --output generated_answers.jsonl --workers 8

# 3. Evaluate performance
raft-eval --question-file evaluation_data/questions.jsonl --answer-file generated_answers.jsonl

# 4. Advanced PromptFlow evaluation
raft-pfeval-chat --input generated_answers.jsonl --output detailed_evaluation.json
```

## üîç Model Comparison Workflow

```bash
# Generate answers with different models
raft-answer --input questions.jsonl --output gpt4_answers.jsonl --model gpt-4
raft-answer --input questions.jsonl --output gpt35_answers.jsonl --model gpt-3.5-turbo

# Evaluate each model
raft-eval --question-file questions.jsonl --answer-file gpt4_answers.jsonl
raft-eval --question-file questions.jsonl --answer-file gpt35_answers.jsonl

# Compare with PromptFlow
raft-pfeval-chat --input gpt4_answers.jsonl --output gpt4_detailed.json
raft-pfeval-chat --input gpt35_answers.jsonl --output gpt35_detailed.json
```

## üê≥ Docker Usage

You can also run the tools in Docker containers:

```bash
# Build container with tools
docker build -t raft-tools .

# Run evaluation
docker run -v $(pwd)/data:/data raft-tools raft-eval --question-file /data/questions.jsonl
```

## üìà Performance Tips

1. **Use multiple workers** (`--workers`) for faster processing
2. **Choose appropriate models** for speed vs. quality trade-offs
3. **Process in batches** for large datasets
4. **Set up proper rate limiting** to avoid API errors
5. **Use local evaluation** when possible to reduce API costs

## üîß Troubleshooting

### Common Issues

**Import Errors**: Ensure all dependencies are installed:
```bash
pip install raft-toolkit[ai,langchain]
```

**API Rate Limits**: Reduce workers or add delays:
```bash
raft-eval --question-file data.jsonl --workers 2
```

**Memory Issues**: Process smaller batches:
```bash
raft-answer --input data.jsonl --count 1000
```

### Getting Help

Use `--help` with any tool for detailed usage information:
```bash
raft-eval --help
raft-answer --help
raft-pfeval-chat --help
```

## üîó Integration with Main Toolkit

The tools integrate seamlessly with the main RAFT Toolkit workflow:

1. Use `raft` to generate datasets
2. Use `raft-answer` to generate model responses
3. Use `raft-eval` for basic evaluation
4. Use `raft-pfeval-*` tools for advanced evaluation

This provides a complete pipeline from dataset generation to comprehensive evaluation and analysis.