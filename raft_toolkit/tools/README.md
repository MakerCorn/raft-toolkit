# 🛠️ RAFT Toolkit - Standalone Tools

> **Comprehensive evaluation and answer generation utilities for RAFT-generated datasets**

This directory contains a collection of powerful standalone tools designed to work seamlessly with datasets generated by the RAFT Toolkit. These tools enable comprehensive model evaluation, answer generation, and performance analysis using various frameworks and methodologies.

---

## 📋 Table of Contents

- [🚀 Quick Start](#-quick-start)
- [🔧 Installation](#-installation)
- [📊 Available Tools](#-available-tools)
  - [📈 eval.py - Dataset Evaluation Tool](#-evalpy---dataset-evaluation-tool)
  - [💬 answer.py - Answer Generation Tool](#-answerpy---answer-generation-tool)
  - [🔍 PromptFlow Evaluation Tools](#-promptflow-evaluation-tools)
- [🏗️ Usage Workflows](#️-usage-workflows)
- [⚙️ Configuration](#️-configuration)
- [📚 Examples](#-examples)
- [🔗 Integration](#-integration)

---

## 🚀 Quick Start

```bash
# Navigate to tools directory
cd tools/

# Install dependencies
pip install -r requirements.txt

# Run basic evaluation
python eval.py --question-file dataset.jsonl --answer-file answers.jsonl
```

---

## 🔧 Installation

### Prerequisites

- Python 3.8+
- OpenAI API key (or Azure OpenAI credentials)
- Core RAFT Toolkit installed

### Quick Installation

```bash
# From the project root directory
cd raft-toolkit/

# Install core dependencies (if not already done)
pip install -r requirements.txt

# Install tools-specific dependencies
cd tools/
pip install -r requirements.txt
```

### Tool-Specific Dependencies

```bash
# For PromptFlow evaluation tools
pip install promptflow-evals azure-ai-ml

# For enhanced evaluation features
pip install tenacity tqdm python-dotenv

# For Azure integration
pip install azure-identity azure-openai

# For local model evaluation
pip install torch transformers

# For advanced analytics
pip install pandas numpy matplotlib seaborn
```

### Verify Installation

```bash
# Test basic evaluation tool
python eval.py --help

# Test answer generation tool
python answer.py --help

# Test PromptFlow tools
python pfeval_chat.py --help
```

### Environment Setup

Create a `.env` file in the tools directory (or use the main project `.env`):

```bash
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_API_BASE_URL=https://api.openai.com/v1

# Azure OpenAI Configuration (optional)
AZURE_OPENAI_ENABLED=false
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_KEY=your_azure_openai_key
AZURE_OPENAI_API_VERSION=2024-02-01

# Evaluation Configuration
EVAL_MODEL=gpt-4
EVAL_WORKERS=4
EVAL_BATCH_SIZE=10
EVAL_TIMEOUT=300

# PromptFlow Configuration (for Azure PromptFlow)
SCORE_AZURE_OPENAI_ENDPOINT=https://your-score-endpoint.openai.azure.com/
SCORE_AZURE_OPENAI_API_KEY=your_score_key
SCORE_OPENAI_API_VERSION=2024-02-01
SCORE_AZURE_OPENAI_DEPLOYMENT=gpt-4

# Azure AI Project Configuration (for PromptFlow)
GROUNDEDNESS_SUB_ID=your_subscription_id
GROUNDEDNESS_GROUP=your_resource_group
GROUNDEDNESS_PROJECT_NAME=your_project_name

REPORT_SUB_ID=your_subscription_id
REPORT_GROUP=your_resource_group
REPORT_PROJECT_NAME=your_project_name
```

### Alternative: Using Ollama (Local Models)

For local model evaluation without API keys:

```bash
# Start Ollama with your model
ollama run llama3

# Set environment variables
export OPENAI_API_BASE_URL="http://localhost:11434/v1"
export OPENAI_API_KEY="ollama-anything"

# Run evaluation tools as normal
python eval.py --question-file dataset.jsonl --model llama3
```

---

## 📊 Available Tools

### 📈 eval.py - Dataset Evaluation Tool

> **Primary evaluation utility for comparing model outputs against reference answers**

#### 🎯 Purpose
Comprehensive evaluation of language model performance on RAFT-generated datasets with support for multiple metrics, parallel processing, and detailed analytics.

#### ⚡ Features
- ✅ **Multi-threaded Processing** - Parallel evaluation for faster results
- ✅ **Rate Limiting** - Built-in retry logic for API calls
- ✅ **Progress Tracking** - Real-time progress with detailed statistics
- ✅ **Flexible I/O** - Customizable input/output column mapping
- ✅ **Usage Analytics** - Token usage and performance metrics

#### 🔧 Configuration

```bash
python eval.py [OPTIONS]
```

**Required Arguments:**
- `--question-file` - Path to JSONL file containing evaluation questions

**Optional Arguments:**
- `--answer-file` - Output file for generated answers (default: `answer.jsonl`)
- `--model` - Model for evaluation (default: `gpt-4`)
- `--input-prompt-key` - Input column name (default: `instruction`)
- `--output-answer-key` - Output column name (default: `answer`)
- `--workers` - Number of worker threads (default: `1`)

#### 📝 Usage Examples

**Basic Evaluation:**
```bash
python eval.py \
  --question-file ../outputs/eval_questions.jsonl \
  --answer-file model_answers.jsonl
```

**Advanced Configuration:**
```bash
python eval.py \
  --question-file evaluation_dataset.jsonl \
  --answer-file gpt4_responses.jsonl \
  --model gpt-4-0125-preview \
  --input-prompt-key question \
  --output-answer-key response \
  --workers 8
```

**Azure OpenAI:**
```bash
# Set environment variables
export AZURE_OPENAI_ENABLED=true
export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com/"

python eval.py \
  --question-file dataset.jsonl \
  --model gpt-4-deployment-name \
  --workers 4
```

#### 📊 Output Format

```json
{
  "instruction": "What is machine learning?",
  "answer": "Machine learning is a subset of artificial intelligence...",
  "timestamp": "2024-01-15T10:30:00Z",
  "model": "gpt-4",
  "tokens_used": 156
}
```

---

### 💬 answer.py - Answer Generation Tool

> **Specialized tool for generating answers to evaluation questions**

#### 🎯 Purpose
Generate high-quality answers for evaluation datasets using various language models with support for custom prompting and batch processing.

#### ⚡ Features
- ✅ **Batch Processing** - Efficient handling of large datasets
- ✅ **Custom Models** - Support for different OpenAI models
- ✅ **Flexible I/O** - Configurable input/output formats
- ✅ **Error Handling** - Robust error recovery and logging
- ✅ **Progress Monitoring** - Real-time progress tracking

#### 🔧 Configuration

```bash
python answer.py [OPTIONS]
```

**Arguments:**
- `--input` - Input JSONL file (default: `input.jsonl`)
- `--output` - Output JSONL file (default: `output.jsonl`)
- `--workers` - Number of worker threads (default: `1`)
- `--model` - Model name for answer generation (default: `gpt-4`)

#### 📝 Usage Examples

**Basic Answer Generation:**
```bash
python answer.py \
  --input questions.jsonl \
  --output answers.jsonl \
  --model gpt-4
```

**High-Throughput Processing:**
```bash
python answer.py \
  --input large_dataset.jsonl \
  --output batch_answers.jsonl \
  --workers 10 \
  --model gpt-3.5-turbo
```

#### 📊 Input Format

```json
{
  "question": "Explain the concept of neural networks",
  "context": "Neural networks are computational models...",
  "id": "q_001"
}
```

#### 📊 Output Format

```json
{
  "question": "Explain the concept of neural networks",
  "context": "Neural networks are computational models...",
  "id": "q_001",
  "answer": "Neural networks are computational models inspired by...",
  "generation_time": "2024-01-15T10:30:00Z"
}
```

---

### 🔍 PromptFlow Evaluation Tools

> **Advanced evaluation suite using Microsoft PromptFlow framework**

#### 🎯 Purpose
Sophisticated evaluation using PromptFlow's built-in evaluators for comprehensive quality assessment across multiple dimensions.

#### ⚡ Available Tools

| Tool | Purpose | Evaluators |
|------|---------|------------|
| `pfeval_chat.py` | Chat format evaluation | Relevance, Groundedness, Fluency, Coherence, Similarity |
| `pfeval_completion.py` | Completion format evaluation | Quality, Accuracy, Completeness |
| `pfeval_local.py` | Local evaluation without API calls | Offline metrics, Custom evaluators |

#### 🔧 Configuration

**Common Arguments:**
- `--input` - Input JSONL file (default: `input.jsonl`)
- `--output` - Output results file (default: `output.jsonl`)
- `--mode` - Evaluation mode: `local` or `remote` (default: `local`)

#### 📝 Usage Examples

**Chat Format Evaluation:**
```bash
python pfeval_chat.py \
  --input chat_dataset.jsonl \
  --output chat_evaluation.json \
  --mode remote
```

**Completion Format Evaluation:**
```bash
python pfeval_completion.py \
  --input completion_dataset.jsonl \
  --output completion_scores.json
```

**Local Evaluation (No API Calls):**
```bash
python pfeval_local.py \
  --input dataset.jsonl \
  --output local_metrics.json \
  --mode local
```

#### 📊 Evaluation Metrics

**Quality Dimensions:**
- 🎯 **Relevance** - How relevant is the answer to the question?
- 🏗️ **Groundedness** - Is the answer grounded in the provided context?
- 💬 **Fluency** - How fluent and natural is the language?
- 🔗 **Coherence** - How coherent and logical is the response?
- 🔍 **Similarity** - How similar is the answer to reference answers?

#### 📊 Output Format

```json
{
  "question_id": "q_001",
  "relevance_score": 4.2,
  "groundedness_score": 4.5,
  "fluency_score": 4.8,
  "coherence_score": 4.3,
  "similarity_score": 0.87,
  "overall_score": 4.36,
  "evaluation_timestamp": "2024-01-15T10:30:00Z"
}
```

---

## 🏗️ Usage Workflows

### 🔄 Complete Evaluation Pipeline

```bash
# 1. Generate dataset with main RAFT toolkit
cd ..
python raft.py --datapath document.pdf --output evaluation_data

# 2. Generate answers
cd tools/
python answer.py \
  --input ../evaluation_data/questions.jsonl \
  --output generated_answers.jsonl \
  --workers 8

# 3. Evaluate performance
python eval.py \
  --question-file ../evaluation_data/questions.jsonl \
  --answer-file generated_answers.jsonl

# 4. Advanced PromptFlow evaluation
python pfeval_chat.py \
  --input generated_answers.jsonl \
  --output detailed_evaluation.json
```

### 🎯 Model Comparison Workflow

```bash
# Generate answers with different models
python answer.py --input questions.jsonl --output gpt4_answers.jsonl --model gpt-4
python answer.py --input questions.jsonl --output gpt35_answers.jsonl --model gpt-3.5-turbo

# Evaluate each model
python eval.py --question-file questions.jsonl --answer-file gpt4_answers.jsonl
python eval.py --question-file questions.jsonl --answer-file gpt35_answers.jsonl

# Compare results
python -c "
import json
with open('gpt4_answers.jsonl') as f: gpt4 = [json.loads(line) for line in f]
with open('gpt35_answers.jsonl') as f: gpt35 = [json.loads(line) for line in f]
print(f'GPT-4 avg length: {sum(len(r[\"answer\"]) for r in gpt4)/len(gpt4)}')
print(f'GPT-3.5 avg length: {sum(len(r[\"answer\"]) for r in gpt35)/len(gpt35)}')
"
```

---

## ⚙️ Configuration

### 🔧 Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_API_KEY` | OpenAI API key | Required |
| `OPENAI_API_BASE_URL` | OpenAI API base URL | `https://api.openai.com/v1` |
| `AZURE_OPENAI_ENABLED` | Enable Azure OpenAI | `false` |
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI endpoint | None |
| `EVAL_MODEL` | Default evaluation model | `gpt-4` |
| `EVAL_WORKERS` | Default worker count | `4` |

### 📁 File Structure

```
tools/
├── README.md              # This documentation
├── __init__.py            # Python package initialization
├── eval.py               # Primary evaluation tool
├── answer.py             # Answer generation tool
├── pfeval_chat.py        # PromptFlow chat evaluation
├── pfeval_completion.py  # PromptFlow completion evaluation
├── pfeval_local.py       # PromptFlow local evaluation
└── requirements.txt      # Tool dependencies
```

---

## 📚 Examples

### 📖 Dataset Formats

**Question File (Input):**
```json
{"instruction": "What is artificial intelligence?", "context": "AI is a field of computer science..."}
{"instruction": "How do neural networks work?", "context": "Neural networks consist of layers..."}
```

**Answer File (Output):**
```json
{"instruction": "What is artificial intelligence?", "answer": "Artificial intelligence is...", "model": "gpt-4"}
{"instruction": "How do neural networks work?", "answer": "Neural networks work by...", "model": "gpt-4"}
```

### 🎨 Custom Evaluation Script

```python
#!/usr/bin/env python3
"""Custom evaluation workflow"""

import subprocess
import json
from pathlib import Path

def run_evaluation_suite(input_file: str, model: str = "gpt-4"):
    """Run complete evaluation suite"""
    
    # Generate answers
    print(f"🤖 Generating answers with {model}...")
    subprocess.run([
        "python", "answer.py",
        "--input", input_file,
        "--output", f"{model}_answers.jsonl",
        "--model", model,
        "--workers", "8"
    ])
    
    # Basic evaluation
    print("📊 Running basic evaluation...")
    subprocess.run([
        "python", "eval.py",
        "--question-file", input_file,
        "--answer-file", f"{model}_answers.jsonl"
    ])
    
    # PromptFlow evaluation
    print("🔍 Running PromptFlow evaluation...")
    subprocess.run([
        "python", "pfeval_chat.py",
        "--input", f"{model}_answers.jsonl",
        "--output", f"{model}_detailed_eval.json"
    ])
    
    print("✅ Evaluation complete!")

if __name__ == "__main__":
    run_evaluation_suite("questions.jsonl", "gpt-4")
```

### 📈 Results Analysis

```python
#!/usr/bin/env python3
"""Analyze evaluation results"""

import json
import numpy as np
from collections import defaultdict

def analyze_results(eval_file: str):
    """Analyze evaluation results"""
    
    with open(eval_file) as f:
        results = [json.loads(line) for line in f]
    
    # Basic statistics
    answer_lengths = [len(r['answer']) for r in results]
    
    print("📊 Evaluation Results Analysis")
    print("=" * 40)
    print(f"Total Responses: {len(results)}")
    print(f"Average Answer Length: {np.mean(answer_lengths):.1f} chars")
    print(f"Median Answer Length: {np.median(answer_lengths):.1f} chars")
    print(f"Answer Length Range: {min(answer_lengths)} - {max(answer_lengths)}")
    
    # Token usage analysis
    if 'tokens_used' in results[0]:
        total_tokens = sum(r.get('tokens_used', 0) for r in results)
        print(f"Total Tokens Used: {total_tokens:,}")
        print(f"Average Tokens per Answer: {total_tokens/len(results):.1f}")

if __name__ == "__main__":
    analyze_results("gpt4_answers.jsonl")
```

---

## 🔗 Integration

### 🔌 With Main RAFT Toolkit

```python
from core.raft_engine import RaftEngine
from tools.eval import get_answer, write_result_to_file

# Generate dataset
engine = RaftEngine(config)
stats = engine.generate_dataset(data_path, output_path)

# Evaluate generated dataset
questions_file = f"{output_path}/questions.jsonl"
python_eval_cmd = f"python tools/eval.py --question-file {questions_file}"
subprocess.run(python_eval_cmd.split())
```

### 🐳 Docker Integration

```dockerfile
# Dockerfile for tools
FROM python:3.11-slim

WORKDIR /app/tools
COPY tools/ .
COPY requirements.txt .

RUN pip install -r requirements.txt

# Run evaluation
ENTRYPOINT ["python", "eval.py"]
```

### ☁️ Cloud Deployment

```yaml
# docker-compose.yml
version: '3.8'
services:
  raft-eval:
    build: 
      context: .
      dockerfile: tools/Dockerfile
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - EVAL_WORKERS=8
    volumes:
      - ./data:/app/data
    command: >
      python eval.py 
      --question-file /app/data/questions.jsonl 
      --answer-file /app/data/answers.jsonl 
      --workers 8
```

---

## 🚀 Performance Tips

### ⚡ Optimization Guidelines

1. **Parallel Processing**: Use `--workers` parameter for faster evaluation
2. **Batch Size**: Process datasets in chunks for memory efficiency  
3. **Model Selection**: Choose appropriate models for speed vs. quality trade-offs
4. **Caching**: Reuse generated answers when possible
5. **Rate Limiting**: Respect API rate limits to avoid errors

### 📊 Benchmarks

| Dataset Size | Workers | Model | Time | Tokens/sec |
|-------------|---------|--------|------|------------|
| 1,000 questions | 4 | GPT-4 | 12 min | 850 |
| 1,000 questions | 8 | GPT-3.5 | 6 min | 1,200 |
| 5,000 questions | 16 | GPT-4 | 45 min | 900 |

---

## 📞 Support

- 📚 **Documentation**: See main RAFT Toolkit README
- 🐛 **Issues**: Report bugs in the main repository
- 💡 **Feature Requests**: Submit enhancement ideas
- 🤝 **Contributing**: Follow main project contribution guidelines

---

*For more information about the RAFT Toolkit, see the main [README.md](../README.md) in the project root.*