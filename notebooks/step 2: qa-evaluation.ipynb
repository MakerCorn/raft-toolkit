{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"output_data_1000\"\n",
    "doc_path = \"../../sample_data/\"\n",
    "ds_path = f\"../../{ds_name}\"\n",
    "print(\"Using dataset: \" + ds_name)\n",
    "\n",
    "raft_arrow_file = f\"{ds_path}/data-00000-of-00001.arrow\"\n",
    "dataset_path = f\"{ds_path}-files/{ds_name}-full.jsonl\"\n",
    "dataset_path_hf = f\"{ds_path}-files/{ds_name}-hf.full.jsonl\"\n",
    "\n",
    "dataset_path_hf_train = f\"{ds_path}-files/{ds_name}-hf.train.jsonl\"\n",
    "dataset_path_hf_valid = f\"{ds_path}-files/{ds_name}-hf.valid.jsonl\"\n",
    "dataset_path_hf_eval = f\"{ds_path}-files/{ds_name}-hf.eval.jsonl\"\n",
    "\n",
    "dataset_path_ft_train = f\"{ds_path}-files/{ds_name}-ft.train.jsonl\"\n",
    "dataset_path_ft_train_filtered = f\"{ds_path}-files/{ds_name}-ft.train.filtered.jsonl\"\n",
    "dataset_path_ft_valid = f\"{ds_path}-files/{ds_name}-ft.valid.jsonl\"\n",
    "dataset_path_ft_valid_filtered = f\"{ds_path}-files/{ds_name}-ft.valid.filtered.jsonl\"\n",
    "dataset_path_ft_eval = f\"{ds_path}-files/{ds_name}-ft.eval.jsonl\"\n",
    "dataset_path_ft_eval_5 = f\"{ds_path}-files/{ds_name}-ft.eval-5.jsonl\"\n",
    "dataset_path_ft_answer = f\"{ds_path}-files/{ds_name}-ft.answer.jsonl\"\n",
    "dataset_path_local_results = f\"{ds_path}-files/{ds_name}.pf-eval-local-results.jsonl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set the Model Name for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"gpt-4-0613.ft-88d65450f4204c35b7857e330659e247\"\n",
    "deployment_name=\"gpt-4-0613-ft-88d65450f4204c35b7857e330659e247\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Generate Answers from the Fine Tuned Chat Completions Model\n",
    "This updates the generated evaluation file which then can be used in AI Studio for quality checks. \n",
    "This step will use the fine-tuned model specified above and process all of the questions and save all of the answers in the evaluation data.\n",
    "This file can then be uploaded to the Evaluation tool in Azure AI Studio to do the evaluation remotely.\n",
    "\n",
    "The system prompt will be loaded from the template file based on the key specified. The default is \"gpt\" and is loaded from gpt_template.txt.\n",
    "\n",
    "To generate a random subset of questions, pass the count parameter to the script. The random set will be limited to questions that do not start with a #.\n",
    "\n",
    "The following environment variables must be set:\n",
    "- EVAL_AZURE_OPENAI_ENDPOINT\n",
    "- EVAL_AZURE_OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 ../answer.py \\\n",
    "--input $dataset_path_ft_eval \\\n",
    "--output $dataset_path_ft_answer \\\n",
    "--model $model_name \\\n",
    "--deployment $deployment_name \\\n",
    "--templates ./ \\\n",
    "--count 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Execute the Azure AI Studio Evaluation Tool locally with the answers file: Run the pfeval-local.py\n",
    "This will run the prompt flow evaluation using the answer file generated in the previous step. This is an alternative to uploading the file to Azure AI Studio.\n",
    "\n",
    "The following environment variables must be set:\n",
    "- SCORE_AZURE_OPENAI_ENDPOINT\n",
    "- SCORE_AZURE_OPENAI_API_KEY\n",
    "- SCORE_AZURE_OPENAI_DEPLOYMENT\n",
    "- GROUNDEDNESS_SUB_ID\n",
    "- GROUNDEDNESS_GROUP\n",
    "- GROUNDEDNESS_PROJECT_NAME\n",
    "- REPORT_SUB_ID\n",
    "- REPORT_GROUP\n",
    "- REPORT_PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 ../pfeval-local.py \\\n",
    "--input $dataset_path_ft_answer \\\n",
    "--output $dataset_path_local_results \\\n",
    "--model_source $model_name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Execute the Azure AI Studio Evalutaion Tool while new target answers for a Chat Completion Model: Run pfeval-chat.py\n",
    "This will run the prompt flow evaluation while retrieving a new answer for each question in the evaluation file.\n",
    "\n",
    "The following environment variables must be set:\n",
    "- EVAL_AZURE_OPENAI_ENDPOINT\n",
    "- EVAL_AZURE_OPENAI_API_KEY\n",
    "- SCORE_AZURE_OPENAI_ENDPOINT\n",
    "- SCORE_AZURE_OPENAI_API_KEY\n",
    "- SCORE_OPENAI_API_VERSION\n",
    "- SCORE_AZURE_OPENAI_DEPLOYMENT\n",
    "- GROUNDEDNESS_SUB_ID\n",
    "- GROUNDEDNESS_GROUP\n",
    "- GROUNDEDNESS_PROJECT_NAME\n",
    "- REPORT_SUB_ID\n",
    "- REPORT_GROUP\n",
    "- REPORT_PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 ../pfeval-chat.py \\\n",
    "--input $dataset_path_ft_eval \\\n",
    "--output $dataset_path_local_results \\\n",
    "--score-model $score_model_name \\\n",
    "--model $model_name \\\n",
    "--deployment $deployment_name \\\n",
    "--templates \"../\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. For Completions Models: Run the eval.py to generate an answers and run the comparison locally.\n",
    "This will run the evaluation locally for a Completions model.\n",
    "\n",
    "The following environment variables must be set:\n",
    "- EVAL_AZURE_OPENAI_ENDPOINT\n",
    "- EVAL_AZURE_OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 ../eval.py \\\n",
    "--question-file $dataset_path_hf_eval \\\n",
    "--answer-file $dataset_path_ft_answer \\\n",
    "--model $model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Execute the Azure AI Studio Evalutaion Tool while new target answers for a Completion Model: Run pfeval.py\n",
    "This will run the prompt flow evaluation retrieving a new answer for each question in the evaluation file. This is an alternative to the eval.py script.\n",
    "\n",
    "The following environment variables must be set:\n",
    "- EVAL_AZURE_OPENAI_ENDPOINT\n",
    "- EVAL_AZURE_OPENAI_API_KEY\n",
    "- SCORE_AZURE_OPENAI_ENDPOINT\n",
    "- SCORE_AZURE_OPENAI_API_KEY\n",
    "- SCORE_OPENAI_API_VERSION\n",
    "- SCORE_AZURE_OPENAI_DEPLOYMENT\n",
    "- GROUNDEDNESS_SUB_ID\n",
    "- GROUNDEDNESS_GROUP\n",
    "- GROUNDEDNESS_PROJECT_NAME\n",
    "- REPORT_SUB_ID\n",
    "- REPORT_GROUP\n",
    "- REPORT_PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 ../pfeval.py \\\n",
    "--input $dataset_path_ft_eval \\\n",
    "--output $dataset_path_local_results \\\n",
    "--score-model $score_model_name \\\n",
    "--model $model_name \\\n",
    "--deployment $deployment_name \\\n",
    "--templates \"../\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
